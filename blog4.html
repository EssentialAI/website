<!DOCTYPE html>
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLEH0W42YV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-QLEH0W42YV');
  </script>
  <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous"> -->

  <link id="mystylesheet" rel="stylesheet" type="text/css" href="light.css">
  <!-- <link rel="stylesheet" href="light.css"> -->
  <link rel="icon" href="brain1.ico" type="image/x-icon">
  <link href="https://fonts.googleapis.com/css2?family=Alegreya+Sans+SC:wght@700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Nunito&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <!-- <script>
    function myFunction() {
      var element = document.body;
      element.classList.toggle("dark-mode");
    }
  </script> -->
  <title>EssentialAI</title>
</head>

<body>
  <div class="topnav">
    <a class="top" href="index.html">Blog</a>
    <a class="top" href="about.html">About</a>
    <!-- <a class="top" href="publications.html">Publications</a> -->
    <!-- <a id="small" href="research.html">Misc. research articles</a> -->
    <button id="dark-mode-toggle" class="dark-mode-toggle">
  <svg width="100%" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 496"><path fill="currentColor" d="M8,256C8,393,119,504,256,504S504,393,504,256,393,8,256,8,8,119,8,256ZM256,440V72a184,184,0,0,1,0,368Z" transform="translate(-8 -8)"/></svg>
</button>
  </div>
  <!-- Start your blog here -->
  <!-- ################################################################################ -->
  <h1 class="bloghead">Logistic Regression from Scratch</h1><br>

  <!-- <p class = "content"><i style="color:red">(Website under development. Articles coming in a couple of days)</i></p> -->
  <!-- <p class="content">The HTML <span>button</span> tag defines a clickable button.</p>
  <p class="content">The CSS <span class="markdown">background-color</span> property defines the background color of an
    element.</p> -->
  <p class="content">Previous articles articulate Linear Regression and Multiple Regression. Practical issues in Machine
    Learning have also been discussed in an earlier article. In this article, let's dive deep into Logistic Regression,
    a building block of Neural Networks, how perception came into existence, the mathematical explanation
    behind Logistic Regression, and finally, the use of Activation functions. As many might know, Machine Learning is
    broadly divided into Supervised and Unsupervised Machine
    Learning. Linear and Logistic Regression fall into the category of Supervised Machine Learning. <span
      class='highlight'>While Linear
      regression, as the name suggests, is a regression model, Logistic Regression is a classification model. Wait!
      Shouldn't it be named 'Logistic Classification'?</span> We shall discuss everything about Logistic Regression in
    this
    article.</p><br>

  <iframe width="750" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/99.embed?showlink=false"></iframe>


  <p class='content'>Let's discuss Linear classification in general. As shown in the above plot, suppose we have some
    points that belong to class 'A' and class 'B.' Our job is to find the line <span class='latex'>\(ax+by+c=0\)</span>
    that best separates these two
    classes. Points that lie on the right side of the line fall into class 'A' and vice versa. Our hypothesis here
    becomes:</p>
  <p class='scenter'><span class='latex'>\(h(x) = w_{0}+w_{1}x_{1}+w_{2}x_{2} = w^TX\)
    </span></p><br>

  <p class='content'>So coming up with a line <span class='latex'>\(ax+by+c=0\)</span> to separate the data points
    sounds like Linear Regression, right. It's true for the most part. Revisiting the equation of the line in Linear
    Regression in its matrix form. We see that using this line for classification becomes useless pretty quickly
    because:</p>
  <p class="highlight"><span class="latex" style="font-size:100%;">\(1.\)</span> We cannot use
    quantitative values for classification as it can take any value on the number line.</p>
  <p class="highlight"><span class="latex" style="font-size:100%;">\(2.\)</span> Linear Regression fails
    to yield the best results when there is no proper correlation in the
    data points, even
    when the decision boundary is perceivable.</p>

  <p class='content'>The way to overcome these problems is by adding an additional function (called activation function)
    after the summation of weights in the feedforward layer. The activation function used in Logistic Regression is
    known as 'sigmoid.'</p>

  <p class='scenter'><span class='latex' style="font-size: 130%;">\(\sigma(x) = \frac{1}{1+e^{-(x)}}\)
    </span></p><br>

  <iframe width="750" height="400" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/79.embed?showlink=false"></iframe>
  <p class='content'>As sigmoid converts any value to lie between 0 and 1, using 0.5 as a threshold, we can easily
    classify data points into different classes. As this process is very similar to that of Linear Regression, the name
    Regression pertains. This is just a simple explanation of Logistic Regression. There is a lot of mathematics and
    probability that goes inside Logistic Regression that makes it so unique. Let's discuss them one by one. </p>

  <p class=" scenter"><span class='sub_head'>Probabilistic Interpretation</span></p>
  <p class='content'>The y coordinate of the sigmoid is considered to represent the Probability Density Function of a
    point belonging to a particular class. This is because a sigmoid always yields a value between 0-1, and a point must
    belong to either of the classes. This means, for two-class classification, if P(y=1/x)=p, then P(y=0/x)=1-p so that
    the sum of all probabilities sum up to one. <span class='highlight'>However, this does not entirely explain using
      the value of p to get the best results out of a Logistic Regression model.</span> To understand this, we must
    understand two things first, the cost function used in Logistic Regression, the concept of Maximum Likelihood.</p>
  <ul>
    <li class='content' style="font-size: 140%;"><strong><span class="highlight">Cross Entropy Loss</span></strong></li>
  </ul>
  <p class="content">As we fit a line just like Linear Regression, can we use the same loss function as Linear
    Regression? No!. The concept of the sum of squared residuals does not work here as we are not trying to fit a line
    that passes through points but to separate them. The cost function we use for logistic regression is called "Cross
    Entropy" and is given by the equation below:</p>
  <p class="latex">\(\begin{align}J=-\sum_{n=1}^{N}t_{n}log(y_{n})+(1-t_{n})log(1-y_{n})\end{align}\)</p>
  <p class="content">Interpreting this cost function is very easy. The main aim of the cost function is to penalize more
    when prediction does not match with actual value.</p>
  <table class="scenter">
    <tr>
      <th>Actual</th>
      <th>Predicted</th>
      <th>Loss</th>
    </tr>
    <tr>
      <td>\(0\)</td>
      <td>\(1\)</td>
      <td>\(-\infty\)</td>
    </tr>
    <tr>
      <td>\(0\)</td>
      <td>\(0\)</td>
      <td>\(0\)</td>
    </tr>
    <tr>
      <td>\(1\)</td>
      <td>\(0\)</td>
      <td>\(\infty\)</td>
    </tr>
    <tr>
      <td>\(1\)</td>
      <td>\(1\)</td>
      <td>\(0\)</td>
    </tr>
  </table>

  <ul>
    <li class='content' style="font-size: 140%;"><strong> <span class="highlight">Maximum Likelihood</span></strong>
    </li>
  </ul>

  <p class="content">It is mind-blowing to know how the output of the sigmoid function is the probability that maximizes
    the accuracy of the decision boundary. Let's consider a simple scenario to understand this. We have a fair coin
    tossed ten times. <span class='highlight'>What is the likelihood that</span> \(7\) <span class='highlight'>heads
      and</span>
    \(3\) <span class="highlight">tails are
      tossed?</span> </p>

  <p class="scenter"><span class="latex">\( \begin{align} P(h)&=p \\ P(t)&=1-P(h) \\ &=1-p\end{align}\)</span></p>

  <p class="scenter"><span class="latex">\( \begin{align} L&=P(h)^{nheads} \cdot
      P(t)^{ntails} \\ &= p^7(1-p)^3\end{align}\)</span></p>

  <p class="scenter"><span class="latex">\( \begin{align} Log(L) = log(p^7(1-p)^3) = 7log(p) +
      3log(1-p)\end{align}\)</span></p>

  <p class='scenter'><span class="highlight">Now, we can use calculus to find the value of p where Maximum likelihood is
      obtained.</span></p>
  <p class="scenter"><span class="latex">\( \begin{align} \frac{\partial l }{\partial p} = 0 \rightarrow p=7/10
      =P(7 \enspace heads/10 \enspace tosses)\end{align}\)</span></p>
  <p class='content'>We see that maximum likelihood is obtained at the probability of heads occurring when a coin is
    tossed ten times. This is the probabilistic explanation of why the output of sigmoid is used as a threshold to
    classify data points. As Maximum Likelihood is obtained at the value of 'p', Logistic decision boundary fits the
    best at that particular value.</p>
  <p class="scenter"><span class="latex">\( \begin{align} P(y=1/x) = \sigma(w^Tx) = y\end{align}\)</span></p>

  <p class='scenter'>How is maximizing log-likelihood the same as minimizing loss?</p>
  <p class="scenter"><span class="latex">\( \begin{align} L = \prod_{n=1}^{N}
      {y_{n}}^{t_{n}}{(1-y_{n})}^{1-t_n}\end{align}\)</span></p>
  <p class="scenter"><span class="latex">\( \begin{align} Log(L) = \sum_{n=1}^{N} t_{n}log(y_{n}) +
      (1-t_n)log(1-y_n)\end{align}\)</span></p>
  <p class='content'>We see that this takes exactly the same equation as the cross-entropy loss funtion except for the
    negative sign. Hence, maximizing log-likelihoood is the same as minimizing cross-entropy loss.</p>
  <!-- <p class='scenter'>Show Equations</p> -->
  <p class=" scenter"><span class='sub_head'>Using Gradient Descent to minimize Cross Entropy</span></p>
  <p class='scenter'>Using Gradient Descent to minimize Cross Entropy</p>
  <p class="latex">\(\begin{align}J=-\sum_{n=1}^{N}t_{n}log(y_{n})+(1-t_{n})log(1-y_{n})\end{align}\)</p>
  <p class="latex">\(\begin{align}\frac{\partial J}{\partial w_i} = \sum_{n=1}^{N}\frac{\partial J}{\partial
    y_n}\frac{\partial y_n}{\partial
    a_n}\frac{\partial a_n}{\partial w_i}\end{align}\)</p>

  <p class="scenter"><span class='highlight'>\(\begin{align}a_n = w^Tx_n\end{align}\)</span></p>
  <p class="scenter"><span class='highlight'>\(\begin{align}y_n = \sigma(a_n) = \frac{1}{1+e^{-a_n}}\end{align}\)</span>
  </p>

  <p class="latex">\(\begin{align}\frac{\partial J}{\partial y_n} =
    -t_n\frac{1}{y_n}+(1-t_n)\frac{1}{1-y_n}(-1)\end{align}\)</p>

  <p class="latex">\(\begin{align}\frac{\partial y_n}{\partial a_n} = \frac{-1}{(1+e^{-a_n})^2}
    (e^{-a_n})(-1)\end{align}\)</p>

  <p class="scenter">\(\begin{align}\frac{\partial y_n}{\partial a_n} = y_n(1-y_n)\end{align}\)
  </p>


  <p class='scenter'> <span class="highlight">Find derivative of \(J\) w.r.t. \(y\) and then derivative of \(y\) w.r.t.
      activation, then activation
      w.r.t. weights \(w\)</span> </p>

  <p class="scenter">\(\begin{align}a_n = w^Tx_n\end{align}\)
  </p>
  <p class="scenter">\(\begin{align}a_n = w_0x_{n0}+w_1x_{n1}+w_2x_{n2}+...\end{align}\)
  </p>

  <p class="scenter">\(\begin{align}\frac{\partial a_n}{\partial w_i} = x_{ni}\end{align}\)
  </p>
  <p class="scenter">\(\begin{align}\frac{\partial J}{\partial w_i} = \sum_{n=1}^{N}(y_n-t_n)x_{ni}\end{align}\)
  </p>

  <p class="scenter">\(\begin{align}\frac{\partial J}{\partial w} =X^T(Y-T)\end{align}\)
  </p>

  <p class="scenter">\(\begin{align}\frac{\partial J}{\partial w_0} =\sum_{n=1}^{N}(y_n-t_n)x_{n0} =
    \sum_{n=1}^{N}(y_n-t_n) \end{align}\)
  </p>
  <p class='scenter'><span class='highlight'>Visualization of reducing loss using Gradient Descent</span></p>

  <iframe width="750" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/83.embed?showlink=false"></iframe>
  <iframe width="750" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/85.embed?showlink=false"></iframe>
  <iframe width="750" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/91.embed?showlink=false"></iframe>

  <p class=" scenter"><span class='sub_head'>Non-Linear Decision Boundaries</span></p>
  <p class='content'>Logistic Regression can also find Non-Linear Decision boundaries. This is what makes logistic
    Regression so special. Just by adding additional features into the hypothesis function \(h(x)\), we can user
    Logistic Regression to find Non-Linear Decision boundaries as shown in the plot below.</p>
  <p class='scenter'>\(h(x) = w_0+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2\)</p>

  <iframe width="750" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/122.embed?showlink=false"></iframe>
  <p class='content'>Visualizing these results in <a
      href="https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4,2&seed=0.75587&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
      style="text-decoration:none;color:#52b788">Tensorflow Playground</a> is a great way to understand the concepts.
    Another problem of finding XOR decision boundary can be done by Logistic Regression alone.</p>
  <p class="content">Complete code for Logistic Regression implementation from scratch available in this <a
      href="https://github.com/Nareshmlblog/EssentialAI-code">repo</a>
  </p>
  <p class="content">Thank you for reading this article.</p>
  <button id="copy_btn">
  </button>
  <p class="scenter" style="font-size: 16px"><a href="https://www.linkedin.com/in/nareshkumar1040/"
      style="text-decoration:none;color:#52b788">NareshKumar Devulapally</a></p>
        <script type="text/javascript">
    // check for saved 'darkMode' in localStorage
let darkMode = localStorage.getItem('darkMode'); 

const darkModeToggle = document.querySelector('#dark-mode-toggle');

const enableDarkMode = () => {
  // 1. Add the class to the body
  document.body.classList.add('darkmode');
  // 2. Update darkMode in localStorage
  localStorage.setItem('darkMode', 'enabled');
}

const disableDarkMode = () => {
  // 1. Remove the class from the body
  document.body.classList.remove('darkmode');
  // 2. Update darkMode in localStorage 
  localStorage.setItem('darkMode', null);
}
 
// If the user already visited and enabled darkMode
// start things off with it on
if (darkMode === 'enabled') {
  enableDarkMode();
}

// When someone clicks the button
darkModeToggle.addEventListener('click', () => {
  // get their darkMode setting
  darkMode = localStorage.getItem('darkMode'); 
  
  // if it not current enabled, enable it
  if (darkMode !== 'enabled') {
    enableDarkMode();
  // if it has been enabled, turn it off  
  } else {  
    disableDarkMode(); 
  }
});


    
  </script>
</body>

</html>