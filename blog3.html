<!DOCTYPE html>
<html>

<head>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-QLEH0W42YV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-QLEH0W42YV');
  </script>
  <!-- <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css" integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous"> -->

  <link id="mystylesheet" rel="stylesheet" type="text/css" href="light.css">
  <!-- <link rel="stylesheet" href="light.css"> -->
  <link rel="icon" href="brain1.ico" type="image/x-icon">
  <link href="https://fonts.googleapis.com/css2?family=Alegreya+Sans+SC:wght@700&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Nunito&display=swap" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Quicksand&display=swap" rel="stylesheet">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <!-- <script>
    function myFunction() {
      var element = document.body;
      element.classList.toggle("dark-mode");
    }
  </script> -->
  <title>EssentialAI</title>
</head>

<body>
  <div class="topnav">
    <a class="top" href="index.html">Blog</a>
    <a class="top" href="about.html">About</a>
    <!-- <a class="top" href="publications.html">Publications</a> -->
    <!-- <a id="small" href="research.html">Misc. research articles</a> -->
    <button id="dark-mode-toggle" class="dark-mode-toggle">
  <svg width="100%" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 496"><path fill="currentColor" d="M8,256C8,393,119,504,256,504S504,393,504,256,393,8,256,8,8,119,8,256ZM256,440V72a184,184,0,0,1,0,368Z" transform="translate(-8 -8)"/></svg>
</button>
  </div>
  <!-- Start your blog here -->
  <!-- ################################################################################ -->
  <h1 class="bloghead">Practical Issues in Machine Learning</h1><br>
  <!-- <p class = "content"><i style="color:red">(Website under development. Articles coming in a couple of days)</i></p> -->
  <!-- <p class="content">The HTML <span>button</span> tag defines a clickable button.</p>
  <p class="content">The CSS <span class="markdown">background-color</span> property defines the background color of an
    element.</p> -->
  <p class="content"> <span class='highlight'">It's not always sunshine and blossoms in the land of Machine Learning.</span> There are many hindrances and blockages in finding the best Machine Learning model for the given dataset. Let's discuss these issues in this article and possible methods to overcome them. When we know that <span class='latex'>\(x^2\)</span> fits a curve better than just using <span class='latex'>\(x\)</span> as a feature, why can't we use <span class='latex'>\(x^3\)</span> and so on as a feature? We can. <span class = 'highlight'>For that matter, we can approximate nearly every function using the Taylor series.</span>  However, we are not looking for a model that best matches the data points. We are looking for a model that provides a good representation of the training data while providing the best insights on future data points. Fitting the model too well on training data and not performing well (or sometimes worse) on the test data is a clear sign of <span class = 'highlight'>Overfitting.</span> (In terms of accuracy, if you see that training accuracy increases towards 100 with epochs while the test accuracy plateaus at around 70, you must understand that something is wrong here.)</p>
  <!-- <h1 class = 'about_head'><span class = 'scenter'>Overfitting</span></h1> -->
  <p class=" scenter"><span class='sub_head'>Overfitting</span></p><br>
  <!-- <iframe src="" frameborder=" 0"></iframe> -->

  <!-- <div class='content'> -->

  <iframe width="650" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/35.embed?showlink=false"></iframe>

  <!-- <div class='scenter'><img src="bias_variance.png" alt=""></div> -->

  <!-- <iframe width="750" height="400" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/39.embed?showlink=false"></iframe> -->
  <!-- <p class="scenter"><span class='highlight' style="font-size: 16px;">Multiple Regression fit on iris
      dataset.</span>
  </p>
  <p class='content'>To understand Multiple Regression, we might have to take a different yet practical approach.
    Multiple Regression can take <span class='latex'>\(D\)</span> features so that the dimensionality can span up to
    <span class='latex'>\(D\)</span> dimensions. This concept can be best understood when equations are represented
    in
    matrix form. Let <span class='latex'>\(N\)</span> be the number of samples in the dataset, <span
      class='latex'>\(D\)</span> be the dimensionality (1 for 1-d Linear Regression, 2 for plane fitting, and so
    on). We
    have to develop a matrix <span class='latex'>\(w\)</span> with the coefficients that provide us with a vector
    <span class='latex'>\(\vec{y}\)</span> that accurately fits given data points. So the equation for Multiple
    Regression
    would be :
  </p> -->
  <p></p>
  <p class='latex'>\(\begin{align}P(x) = \sum_{n=0}^{\infty}\frac{f^{(n)}(a)}{n!}(x-a)^n
    \end{align}\)</p>
  <p class='scenter' style="font-size: 16px; padding-top: 0%;padding-bottom: 0%;">Taylor Series equation for
    approximation of a
    function <span class='latex'>\(P(x)\)</span></p><br>
  <p class='content'>This is also termed as "Generalization error" where in the model is unable to generalize the data.
    Code to demonstrate Overfitting is present in this <a
      href="https://github.com/Nareshmlblog/EssentialAI-code" style="text-decoration: none; color: #2ec4b6;">repo</a> as overfitting.py file. Increase in
    validation loss is another indication that the current model is overfitting.</p>

  <iframe width="650" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/44.embed?showlink=false"></iframe>
  <p class='content'>To understand Overfitting, one must understand the differences between bias and variance first.
    Let's say we have a bunch of data points of a feature <span class='latex'>\(x\)</span> and target <span
      class='latex'>\(y\)</span> plotted over a
    2-d graph. Our goal is to
    formulate the best approximation of the target variable <span class='latex'>\(y\)</span> that depends on <span
      class='latex'>\(x\)</span>. In an
    ideal situation, we have the
    exact mathematical formula for the equation <span class='latex'>\(y=f(x)\)</span>. However, this is a machine
    learning
    test and not a math problem.
    Let's say two students, John and Jason, are asked to develop the best model for these data points. John sees the
    data and uses Linear Regression to fit the data (shown in the left plot), and Jason goes some steps further and uses
    polynomial regression to fit all the data points (shown in the right plot).</p>
  <iframe width="850" height="450" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/47.embed?showlink=false"></iframe>
  <p class='scenter' style="font-size: 16px; padding-top: 0%;padding-bottom: 0%;">Linear Regression v/s Polynomial fit
    on training dataset.
  </p><br>

  <p class='content'>Which model of the two do you think is better? Linear Regression or the Polynomial fit? These
    questions can be answered when we look at how these plots perform with test data. Shown below are charts depicting
    the performance of both these models on the test dataset. We see that Linear Regression performs decently well,
    while the polynomial plot makes pretty huge errors at many places. In other words, Linear Regression, while not
    great, gives us a general representation of the pattern. On the other hand, the Polynomial model performs
    outstandingly well on the training dataset and performs poorly on the test dataset. (The Generalization error we
    talked about).</p>

  <iframe width="850" height="450" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/49.embed?showlink=false"></iframe>
  <p class='scenter' style="font-size: 16px; padding-top: 0%;padding-bottom: 0%;">Performance of Linear Regression and
    Polynomial fit
    on test dataset.
  </p><br>
  <p class='content'>Hence, it is clear that John's model (Linear Regression) is the better model of the two considered
    models. Let's
    understand this in Machine Learning lingo now. As the line produced by Linear Regression cannot be curved, it will
    never be able to capture the true relationship between the data points. <span class='highlight'>This inability of a
      Machine Learning model
      to capture the true relationship is called Bias.</span> So the John's model has higher bias compared to Jason's
    model. In fact the bias in the later model is zero, as the model passes through all the points. We can compare how
    well the models performed on the test dataset using the error function (sum of squared errors) we defined. We see
    that John's model performs way better compared to Jason's model. <span class='highlight'>This difference in
      the performance of models w.r.t. train and test datasets is known as Variance.</span>
  </p>
  <p class="latex">\(\begin{align}Error = \sum_{i=1}^{N}(y_{i}-\hat{y_{i}})^2
    \end{align}\)</p>
  <br>
  <p class='content'>In Machine Learning Lingo, Jason's model has very low bias and high variance in comparison. This
    makes it hard to predict how well Jason's model might perform on future datasets. It might go terribly wrong. In
    contrast, John's model has high bias and low variance. It has a representation of relationship between variables.
    These traits make it a better model, consistently. Hence, Jason's model is said to be Overfit</p>
  <iframe width="950" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/53.embed?showlink=false"></iframe>
  <p class=" scenter"><span class='sub_head'>Methods to overcome Overfitting</span></p><br>
  <p class='content'>Okay! We have figured out that the model is overfiting. What are the next steps to take inorder to
    find the optimal model. Fortunately there are ways to solve this issue. Some of them are discussed below.</p>
  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">Check if all
        Important Features are Included</span>
    </li>
  </ul>
  <p class='content'>Choosing the best parameters that capture all the patterns in the data can be a tedious task. Many
    times many features are simply useless. These features must be removed during the training process to minimize the
    error.</span>
  </p>
  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">Hyperparameter
        Tuning</span>
    </li>
  </ul>
  <p class='content'>Hyperparameters like Learning rate, max_depth (for decision trees), number
      of layers for neural networks, activation function and so on, must be tuned to yield the best possible
    results. All these fixes are towards the model. Lets see what we can do with the data that could help avoid
    Overfitting.
  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">Splitting the
        data</span>
    </li>
  </ul>
  <p class='scenter'><img style="border-radius: 5px;" src="split.gif" alt=""></p>
  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">KFold Cross
        Validation</span>
    </li>
  </ul>
  <p class='content'>Another method that is an extension to splitting data is KFold Cross Validation. The idea here is
    to split the data into batches and training on different sets of batches everytime. This would give a unique
    combination of data everytime. Finally the score attained on all batches is averaged to be the performance of the
    model.</p>
  <p class='scenter'><img style="border-radius: 5px;" src="kfold.PNG" alt=""></p>
  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">Regularization
        methods</span>
    </li>
  </ul>
  <p class='content'>Lets dicuss these in detail here. Lets start with <span class="highlight">L2 Regularization</span>.
    The idea here is that we don't want any outliers in the data because that might make the model shift towards them
    inorder to minimize the loss. Yes we can always remove these outliers, but it is a tedious task finding them. Due to
    these outliers, there is an increase in Bias and Variance of the model. L2 Regularization adds penalty to the loss
    function making the target variable less sensitive towards changes in feature varible. The intuition here is that by
    introducing small amount of bais, we get a significant drop in variance. mathematically speaking, L2 Regularization
    (also called Ridge Regularization), encourages the model weights to be close to zero, while keeping the important
    parameters in place, by adding penalty to the loss
    function. By doing this, the model becomes less sensitive towards outliers.
  </p>
  <iframe width="750" height="500" frameborder="0" scrolling="no"
    src="https://www.plotly.com/~Naresh6017/58.embed?showlink=false"></iframe>
  <p class='scenter' style="font-size: 16px; padding-top: 0%;padding-bottom: 0%;">Effect of outliers on the model
  </p>
  <p class='scenter'>Let's understand the mathematics behind Ridge Regression</p>
  <p class="scenter"><span class='latex'>\(\begin{align}J = \sum_{i=1}^{N}(y_{i}-\hat{y_{i}})^2 + \lambda\left | w
      \right |^2
      \end{align}\)</span></p>
  <p class="scenter"><span class='latex'>\(\begin{align}\left | w \right |^2 = w^Tw =w_{1}^2+w_{2}^2+w_{3}^2+...+w_{D}^2
      \end{align}\)</span></p>
  <p class='scenter'>Recollect from <a href="blog2.html" style="text-decoration: none; color: #2ec4b6;">Multiple Regression article</a> that the loss function <span
      class='latex'>\(J(\theta)\)</span> is given by:</p>
  <p class="scenter"><span class='latex'>\(\begin{align}J = (Y-Xw)^T(Y-Xw)+\lambda w^Tw
      \end{align}\)</span></p>
  <p class="scenter"><span class='latex'>\(\begin{align}J = Y^TY - 2Y^TXw+w^TX^TXw +
      \lambda w^Tw
      \end{align}\)</span></p>
  <p class='scenter'>Minimize new cost function using calculus.</p>
  <p class="scenter"><span class='latex'>\(\begin{align}\frac{\partial J}{\partial w} = -2X^TY+2X^TXw+2\lambda w=0
      \end{align}\)</span></p>

  <p class="scenter"><span class='latex'>\(\begin{align}(\lambda I +X^TX)w =X^TY
      \end{align}\)</span></p>
  <p class="scenter"><span class='latex'>\(\begin{align}w=(\lambda I + X^TX)^{-1}X^TY
      \end{align}\)</span></p>
  <p class='content'>We see that this equation is very similar to the one we obtained during multiple regression. The
    extra term <span class='latex'>\(\lambda I\)</span> penalizes the weights to be close to zero and helps avoid
    overfitting.</p>

  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">L1
        Regularization</span>
    </li>
  </ul>
  <p class='content'>Ideally speaking, we want our input matrix <span class='latex'>\(X\)</span> to be sparse. This
    means that using minimum number of features, we would like to generate a model that best generalizes the pattern in
    the data. This would require a regularization to aggressively to find the most important features. This is exactly
    what L1 Regularization does.</p>
  <p class='content'>If <span class='latex'>\(N\)</span> is the number of samples in the data and <span
      class='latex'>\(D\)</span> be the number of dimensions. The matrix <span class='latex'>\(X\)</span> is denoted as
    follows:</p>
  <p class="scenter"><span class='latex'>\(\begin{align}X = \begin{bmatrix}
      x_{11} & x_{12} & x_{13} & ... & x_{1D} \\
      x_{21} & x_{22} & x_{23} & ... & x_{2D} \\
      x_{31} & x_{32} & x_{33} & ... & x_{3D} \\
      ... & ... & ... & ... & ... \\
      x_{N1} & x_{N2} & x_{N3} & ... & x_{ND}
      \end{bmatrix}
      \end{align}\)</span></p>
  <p class='scenter'> <span class='highlight'>Ideally we would want <span class='latex'>\(D << N\)</span></span>
  </p>
  <p class="scenter"><span class='latex'>\(\begin{align}J = \sum_{i=1}^{N}(y_{i}-\hat{y_{i}})^2 + \lambda\left | w
      \right |
      \end{align}\)</span></p>
  <p class="scenter"><span class='latex'>\(\begin{align}J = (Y-Xw)^T(Y-Xw)+\lambda\left | w
      \right |
      \end{align}\)</span></p>

  <p class="scenter"><span class='latex'>\(\begin{align}J = Y^TY - 2Y^TXw+w^TX^TXw +
      \lambda\left | w
      \right |
      \end{align}\)</span></p>
  <p class="scenter"><span class='latex'>\(\begin{align}\frac{\partial J}{\partial w} = -2X^TY+2X^TXw+\lambda sign(w)=0
      \end{align}\)</span></p>
  <p class='scenter'><span class="highlight"><span class='latex'>\(sign(x)\)</span> is given by:</span></p>

  <p class="scenter"><span class='latex'>\(\begin{align}sign(x) = \left\{\begin{matrix}
      \enspace 1\enspace \enspace if \enspace x>0\\
      -1\enspace if \enspace x<0\\ 0\enspace \enspace if \enspace x=0\\ \end{matrix}\right. \end{align}\)</span>
  </p>
  <p class='content'>We can't solve this problem directly as direct differentiation is not possible. Hence, we have to
    use Gradient Descent to solve for optimal weights. After solving for w many of the weights will be equated to zero.
  </p>
  <ul class='content'>
    <li> <span class='highlight' style="font-size: 22px; font-family: 'Alegreya Sans SC', sans-serif;">L1 v/s L2
        Regularization Visual Difference</span>
    </li>
  </ul>
  <div class='scenter'><iframe width="750" height="500" frameborder="0" scrolling="no"
      src="https://www.plotly.com/~Naresh6017/61.embed?showlink=false"></iframe></div>
  <p class="content">Important point to takeaway here is that derivative of a quadratic function is small for smaller
    values of x but is not absolutely zero. In contrast for values closer to zero derivate of sign function is absolute
    zero thus totally removing the effect of weights on the model. Hence making the <span class='latex'>\(X\)</span>
    matrix sparse. L2 regularization encourages to be smaller and smaller while L1 regularization encourages weights to
    be zero.</p>
  <p class='content'>Thank you for reading this article.</p>
  </p>




  <!-- <div class='scenter'><img src="bias_variance.png" alt="" width="500" height="400"></div> -->
  <button id="copy_btn">
  </button>
  <p class="scenter" style="font-size: 16px"><a href="https://www.linkedin.com/in/nareshkumar1040/"
      style="text-decoration:none;color:#52b788">NareshKumar Devulapally</a></p>
        <script type="text/javascript">
    // check for saved 'darkMode' in localStorage
let darkMode = localStorage.getItem('darkMode'); 

const darkModeToggle = document.querySelector('#dark-mode-toggle');

const enableDarkMode = () => {
  // 1. Add the class to the body
  document.body.classList.add('darkmode');
  // 2. Update darkMode in localStorage
  localStorage.setItem('darkMode', 'enabled');
}

const disableDarkMode = () => {
  // 1. Remove the class from the body
  document.body.classList.remove('darkmode');
  // 2. Update darkMode in localStorage 
  localStorage.setItem('darkMode', null);
}
 
// If the user already visited and enabled darkMode
// start things off with it on
if (darkMode === 'enabled') {
  enableDarkMode();
}

// When someone clicks the button
darkModeToggle.addEventListener('click', () => {
  // get their darkMode setting
  darkMode = localStorage.getItem('darkMode'); 
  
  // if it not current enabled, enable it
  if (darkMode !== 'enabled') {
    enableDarkMode();
  // if it has been enabled, turn it off  
  } else {  
    disableDarkMode(); 
  }
});


    
  </script>
</body>

</html>